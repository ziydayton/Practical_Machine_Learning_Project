---
title: "Pract Mach Learning"
author: "J. Dayton"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

This 

## Load Library
```{r libs, warning=FALSE, echo=FALSE, include=FALSE}
library(caret)
library(dplyr)
library(knitr)
library(RCurl)
```

## Load Data

The data was loaded from the sources:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r loadData, echo=FALSE}
trainURL <- getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
validURL <- getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)

initData <- read.csv(textConnection(trainURL), header=T, stringsAsFactors = TRUE)
valid <- read.csv(textConnection(validURL), header=T, stringsAsFactors = TRUE)

rm(trainURL, validURL)
```

## Cross Validation
The original dataset was randomly sliced into two parts using 70% for a training set and the remaining 30% for a test set.  Additionally, a set of 20 obseravtions was provided for a final validation set (aka for the Quiz!). The training set was used to fit the models and the test set was used for model selection based on the accuracy to account for the out-of-sample error. The summary table of the dimensions for the initial data, training set, testing set, and validation set is provided below:
```{r partData, include=FALSE}
set.seed(4949)
tIndex <- createDataPartition(initData$classe, p = 0.7, list=FALSE)
training <- initData[tIndex,]
testing <- initData[-tIndex,]
#dimension of initial, training, testing, and validation datasets
df <- rbind("Initial data" = dim(initData),"Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df; rm(tIndex, df)
```

## Clean Data

The a seperate process of exploratory data analysis, two primary issues were noted with repesct to the data set.  (1) The data contains varibles for identification such as the participant, index, and timestamp.  (2) The data contains variables with many missing, _NA_, values.

### Assumptions

(1) The indentification variables are not applcable to the prediction.
(2) Columns with _greater the 90%_ of content missing are not useful for prediction.

The following table indicates the dimensionality of each data set after the identification variable are removed.
```{r initTreat, echo=FALSE}
training <- training[, -c(1:5)]
testing <- testing[, -c(1:5)]
valid <- valid[, -c(1:5)]
df <- rbind("Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df; rm(df)
```

Next, the variable (columns) lacking more than 90% of information _(i.e. >90% NA)_ were removed.  The folling table displays the dimensons of the final treated data sets.
```{r treatComp, echo=FALSE}
#Find the columns that have majority NA values
trainNA <- apply(training, 2, function(x) mean(is.na(x))) > 0.90
testNA <- apply(testing, 2, function(x) mean(is.na(x))) > 0.90
validNA <- apply(valid, 2, function(x) mean(is.na(x))) > 0.90
#Find the column names for the NA values
trainNA <- names(which(trainNA))
testNA <- names(which(testNA))
validNA <- names(which(validNA))
#Find all the unique colnames
uniqNA <- unique(c(trainNA, testNA, validNA))
#Remove the 'bad' columns from the data sets
training <- training %>% select(-one_of(uniqNA))
testing <- testing %>% select(-one_of(uniqNA))
valid <- valid %>% select(-one_of(uniqNA))
#Check dims (columns match, rows do not)
df <- rbind("Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df
sum(is.na(training), is.na(testing), is.na(valid)); rm(trainNA, testNA, validNA, uniqNA, df)
```

## Create models
```{r modsFit}
set.seed(16853)
#predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model. 
# mRF <- train(classe ~ ., data = training, method = "rf", prox = TRUE)
# mGBM <- train(diagnosis ~ ., data = training, method = "gbm", verbose = FALSE)
# mLDA <- train(diagnosis ~ ., data = training, method = "lda", verbose = FALSE)
# 
# pRF <- predict(modFitRF, newdata = testing)
# pGBM <- predict(modFitGBM, newdata = testing)
# pLDA <- predict(modFitLDA, newdata = testing)
# 
# aRF <- confusionMatrix(pRF, testing$diagnosis)$overall[1]
# aGBM <- confusionMatrix(pGBM, testing$diagnosis)$overall[1]
# aLDA <- confusionMatrix(pLDA, testing$diagnosis)$overall[1]
# 
# #Stack the predictions together using random forests ("rf"). 
# #What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# predDF <- data.frame(pRF, pGBM, pLDA, diagnosis = testing$diagnosis)
# mAll <- train(diagnosis ~ ., data = predDF, method = "rf", prox = TRUE)
# pAll <- predict(mAll, newdata = predDF)
# aAll <- confusionMatrix(pAll, testing$diagnosis)$overall[1]
# 
# #ANSWER
# aRF
# aGBM
# aLDA
# aAll
```
