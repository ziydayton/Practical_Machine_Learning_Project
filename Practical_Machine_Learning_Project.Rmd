---
title: "Pract Mach Learning"
author: "J. Dayton"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

This 

## Load Library
```{r libs, echo=FALSE, warning=FALSE, include=FALSE}
library(caret)
library(dplyr)
library(knitr)
library(RCurl)
```

## Load Data

The data was loaded from the sources:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r loadData, echo=FALSE}
f <- "pml-training.csv"
if(file.exists(f)) {
   initData <- read.csv(f, header = TRUE, stringsAsFactors = TRUE)
} else {
   trainURL <- getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
   initData <- read.csv(textConnection(trainURL), header = TRUE, 
                     stringsAsFactors = TRUE)
   rm(trainURL)
}

f <- "pml-testing.csv"
if(file.exists(f)) {
   valid <- read.csv(f, header = TRUE, stringsAsFactors = TRUE)
} else {
   validURL <- getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
   valid <- read.csv(textConnection(validURL), header=TRUE,
                  stringsAsFactors = TRUE)
   rm(validURL)
}
rm(f)
```

## Cross Validation
The original dataset was randomly sliced into two parts using 70% for a training set and the remaining 30% for a test set.  Additionally, a set of 20 obseravtions was provided for a final validation set (aka for the Quiz!). The training set was used to fit the models and the test set was used for model selection based on the accuracy to account for the out-of-sample error. The summary table of the dimensions for the initial data, training set, testing set, and validation set is provided below:
```{r partData, echo=FALSE}
set.seed(4949)
tIndex <- createDataPartition(initData$classe, p = 0.7, list=FALSE)
training <- initData[tIndex,]
testing <- initData[-tIndex,]
#dimension of initial, training, testing, and validation datasets
df <- rbind("Initial data" = dim(initData),"Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df; rm(tIndex, df)
```

## Clean Data

The a seperate process of exploratory data analysis, two primary issues were noted with repesct to the data set.  (1) The data contains varibles for identification such as the participant, index, and timestamp.  (2) The data contains variables with many missing, _NA_, values.

### Assumptions

(1) The indentification variables are not applcable to the prediction.
(2) Columns with _greater the 90%_ of content missing are not useful for prediction.

The following table indicates the dimensionality of each data set after the identification variable are removed.
```{r initTreat, echo=FALSE}
training <- training[, -c(1:5)]
testing <- testing[, -c(1:5)]
valid <- valid[, -c(1:5)]
df <- rbind("Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df; rm(df)
```

Next, the variable (columns) lacking more than 90% of information _(i.e. >90% NA)_ were removed.  The folling table displays the dimensons of the final treated data sets.
```{r treatComp, echo=FALSE}
#Find the columns that have majority NA values
trainNA <- apply(training, 2, function(x) mean(is.na(x))) > 0.90
testNA <- apply(testing, 2, function(x) mean(is.na(x))) > 0.90
validNA <- apply(valid, 2, function(x) mean(is.na(x))) > 0.90
#Find the column names for the NA values
trainNA <- names(which(trainNA))
testNA <- names(which(testNA))
validNA <- names(which(validNA))
#Find all the unique colnames
uniqNA <- unique(c(trainNA, testNA, validNA))
#Remove the 'bad' columns from the data sets
training <- training %>% select(-one_of(uniqNA))
testing <- testing %>% select(-one_of(uniqNA))
valid <- valid %>% select(-one_of(uniqNA))
#Check dims (columns match, rows do not)
df <- rbind("Training" = dim(training), 
            "Testing" = dim(testing), "Validation" = dim(valid))
colnames(df) <- c("row", "column")
df
sum(is.na(training), is.na(testing), is.na(valid)); rm(trainNA, testNA, validNA, uniqNA, df)
```

## Create models
```{r modsFit, echo=FALSE}
set.seed(16853)
#predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model.
pt <- proc.time()
#Random Forrest
if(file.exists("mRF.RData")) {
   load(file="mRF.RData", verbose=FALSE)
} else {
   trCtrlRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
   mRF <- train(classe ~ ., data = training, method = "rf", 
                trControl = trCtrlRF)
   save(mRF, file = "mRF.RData")
}
proc.time() - pt

#Boosted Trees
if(file.exists("mGBM.RData")) {
   load(file="mGBM.RData", verbose=FALSE)
} else {
   mGBM <- train(classe ~ ., data = training, method = "gbm", verbose = FALSE)
   save(mGBM, file = "mGBM.RData")
}
proc.time() - pt

#Linear Discriminant Analysis
if(file.exists("mLDA.RData")) {
   load(file="mLDA.RData", verbose=FALSE)
} else {
   mLDA <- train(classe ~ ., data = training, method = "lda", verbose = FALSE)
   save(mLDA, file = "mLDA.RData")
}
proc.time() - pt

# Predict on the test set
pRF <- predict(mRF, newdata = testing)
pGBM <- predict(mGBM, newdata = testing)
pLDA <- predict(mLDA, newdata = testing)

#Find the accuracy on the test set for model selection
aRF <- confusionMatrix(pRF, testing$classe)$overall[1]
aGBM <- confusionMatrix(pGBM, testing$classe)$overall[1]
aLDA <- confusionMatrix(pLDA, testing$classe)$overall[1]
# 
# #Stack the predictions together using random forests ("rf"). 
# #What is the resulting accuracy on the test set? Is it better or worse than each of the individual predictions?
# predDF <- data.frame(pRF, pGBM, pLDA, diagnosis = testing$diagnosis)
# mAll <- train(diagnosis ~ ., data = predDF, method = "rf", prox = TRUE)
# pAll <- predict(mAll, newdata = predDF)
# aAll <- confusionMatrix(pAll, testing$diagnosis)$overall[1]

print(paste0("The Random Forrest out-of-sample accuracy: ", round(aRF * 100, 3), "%"))
print(paste0("The Boosted Trees out-of-sample accuracy: ", round (aGBM * 100, 3), "%"))
print(paste0("The Linear Discriminant Analysis out-of-sample accuracy: ", round(aLDA * 100, 3), "%"))
```

## Model Selection
The resulting model accuracy for Random Forrest was the highest at 99.643% vice the Boosted Trees at 98.505% and Linear Discriminate Analysis ata 70.756%.  Random Forrest was select for the validation run.

Using the Random Forrest model, the following prediction resulted:
```{r validPred, echo=FALSE}
predValidRF <- predict(mRF, newdata = valid)
df <- data.frame(Case = c(1:20), Pred_Exer = predValidRF); df
```
This prediction resulted in 100% Accuracy on the validation data set (e.g. the final quiz). 